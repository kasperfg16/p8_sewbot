gradient\_steps:1\\
batch\_size:20\\
discount\_factor:0.99\\
polyak:0.005\\
actor\_learning\_rate:0.001\\
critic\_learning\_rate:0.001\\
learning\_rate\_scheduler:None\\
learning\_rate\_scheduler\_kwargs:\\
state\_preprocessor:None\\
state\_preprocessor\_kwargs:\\
random\_timesteps:0\\
learning\_starts:20\\
grad\_norm\_clip:0\\
exploration:\\
{ \phantom - } noise:skrl.resources.noises.torch.gaussian.GaussianNoise\\
{ \phantom - } initial\_scale:1.0\\
{ \phantom - } final\_scale:0.001\\
{ \phantom - } timesteps:None\\
rewards\_shaper:None\\
experiment:\\
{ \phantom - } directory:runs_for_report\\
{ \phantom - } experiment\_name:DDPG_env_iteration_4_config_1\\
{ \phantom - } write\_interval:101\\
{ \phantom - } checkpoint\_interval:500\\
{ \phantom - } store\_separately:False\\
{ \phantom - } wandb:False\\
{ \phantom - } wandb\_kwargs:\\

iteration 4:

    change to env:

    # Do not give coverage reward if it has not moved camera check first will ensure to not start the agent task in a real scenario
                if self.step_counter > 0:
                    coveragereward = self.get_coverage() # output percentage